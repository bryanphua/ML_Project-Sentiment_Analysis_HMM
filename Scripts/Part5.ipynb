{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: An improved sentiment analysis system\n",
    "\n",
    "## (10 pts) Now, based on the training and development set, think of a better design for developing an improved sentiment analysis system for tweets using any model you like. \n",
    "\n",
    "In the previous parts, we have adopted the use of the Hidden Markov Models (HMM) for the sentiment analysis system.\n",
    "\n",
    "In this challenge, we have decided to use a different model for the system. The model we have decided to use is Conditional Random Fields (CRF). \n",
    "\n",
    "A CRF can be considered as a generalization of HMM or we can say that a HMM is a particular case of CRF where constant probabilities are used to model state transitions. \n",
    "\n",
    "In contrast to the generative model (HMM), CRF is a discriminative model and the primary advantage of CRFs over HMMs is their conditional nature, resulting in the relaxation of the independence assumptions required by HMMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data and is great for natural language processing.\n",
    "\n",
    "pycrfsuite is a python binding to CRFsuite - an implementation of Conditional Random Fields (CRFs) for labeling sequential data.\n",
    "https://github.com/scrapinghub/python-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training\n",
    "\n",
    "We have two separate functions for obtaining data, one for the training data (labelled) and one for the test data (unlabelled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    f = open(filename,'r')\n",
    "    lines = f.readlines()\n",
    "    datas = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i] == '\\n':\n",
    "            datas.append(lines[start:i])\n",
    "            start = i+1\n",
    "        lines[i] = lines[i].replace('\\n','')\n",
    "        lines[i] = tuple(lines[i].split(' '))\n",
    "        \n",
    "    # check formatting\n",
    "    for i in range(len(datas)):\n",
    "        for j in range(len(datas[i])):\n",
    "            assert len(datas[i][j])==2\n",
    "    \n",
    "    return datas\n",
    "\n",
    "def get_unlabelled_data(filename):\n",
    "    f = open(filename,'r')\n",
    "    lines = f.readlines()\n",
    "    datas = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i] == '\\n':\n",
    "            datas.append(lines[start:i])\n",
    "            start = i+1\n",
    "        lines[i] = lines[i].replace('\\n','')\n",
    "    \n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Part-of-Speech Tags\n",
    "\n",
    "First, we use a feature in NLP: Part-of-Speech (POS) tagging of the words. These tags indicate whether the word is a noun, verb or an adjective etc. \n",
    "\n",
    "NLTK's POS tagger will be used to generate the POS tags for the data.\n",
    "\n",
    "*Note that there is a slight difference in POS-tagging for labelled and unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP', 'O'), ('were', 'VBD', 'O'), ('then', 'RB', 'O'), ('charged', 'VBN', 'O'), ('for', 'IN', 'O'), ('their', 'PRP$', 'O'), ('most', 'RBS', 'O'), ('expensive', 'JJ', 'O'), ('sake', 'NN', 'O'), ('(', '(', 'O'), ('$', '$', 'O'), ('20', 'CD', 'O'), ('+', 'NNP', 'O'), ('per', 'IN', 'O'), ('serving', 'VBG', 'O'), (')', ')', 'O'), ('when', 'WRB', 'O'), ('we', 'PRP', 'O'), ('in', 'IN', 'O'), ('fact', 'NN', 'O'), ('drank', 'IN', 'O'), ('a', 'DT', 'O'), ('sake', 'NN', 'O'), ('of', 'IN', 'O'), ('less', 'JJR', 'O'), ('than', 'IN', 'O'), ('half', 'PDT', 'O'), ('that', 'DT', 'O'), ('price', 'NN', 'O'), ('.', '.', 'O')]\n",
      "\n",
      "BREAK\n",
      "\n",
      "[('When', 'WRB'), ('I', 'PRP'), ('called', 'VBD'), ('this', 'DT'), ('morning', 'NN'), (',', ','), ('I', 'PRP'), (\"didn't\", 'VBP'), ('think', 'VB'), ('I', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('get', 'VB'), ('in', 'IN'), ('at', 'IN'), ('12', 'CD'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('able', 'JJ'), ('to', 'TO'), ('get', 'VB'), ('in', 'IN'), (',', ','), ('along', 'IN'), ('with', 'IN'), ('four', 'CD'), ('other', 'JJ'), ('guests', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def pos_tagging(datas):\n",
    "    data = []\n",
    "\n",
    "    for i, sentences in enumerate(datas):\n",
    "        \n",
    "        # Obtain the list of tokens in the data\n",
    "        tokens = [t for t, label in sentences]\n",
    "\n",
    "        # Perform POS tagging\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "        # Take the word, POS tag, and its label\n",
    "        data.append([(w, POS, label) for (w, label), (word, POS) in zip(sentences, tagged)])\n",
    "        \n",
    "    return data\n",
    "\n",
    "def unlabelled_pos_tagging(datas):\n",
    "    data = []\n",
    "    \n",
    "    for i, sentences in enumerate(datas):\n",
    "        \n",
    "        # Obtain the list of tokens in the data\n",
    "        tokens = [t for t in sentences]\n",
    "        \n",
    "        # Perform POS tagging\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        \n",
    "        # Take the word and its POS tag\n",
    "        data.append(tagged)\n",
    "        \n",
    "    return data\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Example of data after POS-tagging\n",
    "\"\"\"\n",
    "datas = get_data('EN/train')\n",
    "tagged_data = pos_tagging(datas)\n",
    "print(tagged_data[0])\n",
    "\n",
    "print(\"\\n\" + \"BREAK\" + \"\\n\")\n",
    "\n",
    "datas = get_unlabelled_data('EN/dev.in')\n",
    "tagged_data = unlabelled_pos_tagging(datas)\n",
    "print(tagged_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Features\n",
    "\n",
    "POS tag is one of the features for each of the token. However, we require more features in the dataset for better accuracy.\n",
    "\n",
    "We have adopted some of the more commonly used features for a word in named entity recognition:\n",
    "\n",
    "    The word (w) itself (converted to lowercase for normalisation)\n",
    "    The prefix/suffix of w (e.g. -ion)\n",
    "    The words surrounding w, such as the previous and the next word\n",
    "    Whether w is in uppercase or lowercase\n",
    "    Whether w is a number, or contains digits\n",
    "    The POS tag of w, and those of the surrounding words\n",
    "    Whether w is or contains a special character (e.g. hypen, dollar sign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    POStag = sentence[i][1]\n",
    "\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'POStag=' + POStag\n",
    "    ]\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a sentence\n",
    "    if i > 0:\n",
    "        word1 = sentence[i-1][0]\n",
    "        POStag1 = sentence[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:POStag=' + POStag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a sentence'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a sentence\n",
    "    if i < len(sentence)-1:\n",
    "        word1 = sentence[i+1][0]\n",
    "        POStag1 = sentence[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:POStag=' + POStag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "To train the model, we need to first prepare the training data and the corresponding labels. \n",
    "\n",
    "We separate the data into two parts: features & labels - by extracting them from the data.\n",
    "\n",
    "The process_data function serves as a form of convenience for getting our x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for extracting features in sentences\n",
    "def extract_features(sentence):\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "# A function for generating the list of labels for each sentence\n",
    "def get_labels(sentence):\n",
    "    return [label for (token, POStag, label) in sentence]\n",
    "\n",
    "# A function for getting x_train and y_train\n",
    "def process_data(filename, labelled = True):\n",
    "    if (labelled):\n",
    "        datas = get_data(filename)    \n",
    "        tagged_data = pos_tagging(datas)\n",
    "        x = [extract_features(sentence) for sentence in tagged_data]\n",
    "        y = [get_labels(sentence) for sentence in tagged_data]\n",
    "    else:\n",
    "        datas = get_unlabelled_data(filename)\n",
    "        tagged_data = unlabelled_pos_tagging(datas)\n",
    "        x = [extract_features(sentence) for sentence in tagged_data]\n",
    "        y = 0\n",
    "        \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model using pycrfsuite.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(filename, model_out):\n",
    "    x_train, y_train = process_data(filename)\n",
    "    \n",
    "    # Set (verbose=True) to see the steps in training the model\n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "    # Submit training data to the trainer\n",
    "    for xseq, yseq in zip(x_train, y_train):\n",
    "        trainer.append(xseq, yseq)\n",
    "\n",
    "    # Set the parameters of the model\n",
    "    trainer.set_params({\n",
    "        # coefficient for L1 penalty\n",
    "        'c1': 0.1,\n",
    "\n",
    "        # coefficient for L2 penalty\n",
    "        'c2': 0.01,  \n",
    "\n",
    "        # maximum number of iterations\n",
    "        'max_iterations': 200,\n",
    "\n",
    "        # whether to include transitions that\n",
    "        # are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "    # model will be trained and output to the file as specified in the argument\n",
    "    trainer.train(model_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply CRF on Test Data\n",
    "\n",
    "Using the trained model, we apply it to the test data to predict the labels for each word in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_file(fin, fout, model):\n",
    "    x_test, y_test = process_data(fin, labelled=False)\n",
    "    unlabelled_data = get_unlabelled_data(fin)\n",
    "    \n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(model)\n",
    "    y_pred = [tagger.tag(xseq) for xseq in x_test]\n",
    "    \n",
    "    fout = open(fout,'w')\n",
    "    for i in range(len(unlabelled_data)):\n",
    "        for j in range(len(unlabelled_data[i])):\n",
    "            x = unlabelled_data[i][j]\n",
    "            y = y_pred[i][j]\n",
    "            fout.write('{} {}\\n'.format(x,y))\n",
    "        fout.write('\\n')\n",
    "    fout.close\n",
    "    print(\"Conditional Random Fields complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Decoding on EN data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "decoding ...\n",
      "Conditional Random Fields complete\n"
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "trained_model = train_model('EN/train', 'crf_EN.model')\n",
    "\n",
    "print('decoding ...')\n",
    "decode_file('EN/dev.in','EN/dev.p5.out', 'crf_EN.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    >python3 evalResult.py EN/dev.out EN/dev.p5.out\n",
    "\n",
    "    #Entity in gold data: 226\n",
    "    #Entity in prediction: 179\n",
    "\n",
    "    #Correct Entity : 128\n",
    "    Entity  precision: 0.7151\n",
    "    Entity  recall: 0.5664\n",
    "    Entity  F: 0.6321\n",
    "\n",
    "    #Correct Sentiment : 87\n",
    "    Sentiment  precision: 0.4860\n",
    "    Sentiment  recall: 0.3850\n",
    "    Sentiment  F: 0.4296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Decoding on FR data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "decoding ...\n",
      "Conditional Random Fields complete\n"
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "trained_model = train_model('FR/train', 'crf_FR.model')\n",
    "\n",
    "print('decoding ...')\n",
    "decode_file('FR/dev.in','FR/dev.p5.out', 'crf_FR.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "    >python3 evalResult.py FR/dev.out FR/dev.p5.out\n",
    "\n",
    "    #Entity in gold data: 223\n",
    "    #Entity in prediction: 181\n",
    "\n",
    "    #Correct Entity : 144\n",
    "    Entity  precision: 0.7956\n",
    "    Entity  recall: 0.6457\n",
    "    Entity  F: 0.7129\n",
    "\n",
    "    #Correct Sentiment : 106\n",
    "    Sentiment  precision: 0.5856\n",
    "    Sentiment  recall: 0.4753\n",
    "    Sentiment  F: 0.5248"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 pts) We will evaluate your system’s performance on two held out test sets EN/test.in and FR/test.in. The test sets will only be released on 4 Dec 2017 at 5pm (48 hours before the deadline). Use your new system to generate the outputs. Write your outputs to EN/test.p5.out and FR/test.p5.out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Decoding on test(EN) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "decoding ...\n",
      "Conditional Random Fields complete\n"
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "trained_model = train_model('EN/train', 'crf_EN.model')\n",
    "\n",
    "print('decoding ...')\n",
    "decode_file('test/EN/test.in','test/EN/test.p5.out', 'crf_EN.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Decoding on test(FR) data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "decoding ...\n",
      "Conditional Random Fields complete\n"
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "trained_model = train_model('FR/train', 'crf_FR.model')\n",
    "\n",
    "print('decoding ...')\n",
    "decode_file('test/FR/test.in','test/FR/test.p5.out', 'crf_FR.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
